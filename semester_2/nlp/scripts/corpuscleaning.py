# -*- coding: utf-8 -*-
"""CorpusCleaning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tcQL35EyrlLs5jZNo88jr9Fn7G5bQnf7

Corpus Cleaning techniques:
1. lowercasing - convert the text to lowercase.
2. stemming - removing the endings of words to try to convert the word to its base form.
3. lemmatization - convert a word to its base form (this may need some external resources).
4. stop word removal - removes common function words to aid in information retrieval.
5. spelling correction - corrects spellings.
"""

input = "The rain in Spain stays mainly in the plain"

def countUniqueWords(text):
	words = text.split(" ")
	vocab = set()
	for word in words:
		vocab.add(word)
	print(len(vocab))

countUniqueWords(input)

input = "The rain in Spain stays mainy in the plain. What a glorious feeling of singing in the rain. I am singing, and dancing in the rain."
#lowercasing
#tokenization
#stemming
#lemmatization
#stop word removal
#spelling correction

#Lowercasing
lowercasedText = input.lower()
print(lowercasedText)

import nltk
from nltk import sent_tokenize, word_tokenize
nltk.download('punkt_tab')
for sentence in sent_tokenize(input):
  print(sentence.lower())
  for word in word_tokenize(sentence):
    print(word.lower(), end=" ")
  print()

inputTest = "Dr. Sandeep Albert Mathias"
print(len(word_tokenize(inputTest)))

#Stemming
from nltk.stem import PorterStemmer
stemmer = PorterStemmer()

for sentence in sent_tokenize(input):
  for word in word_tokenize(sentence):
    print(word, "->", stemmer.stem(word, to_lowercase=False))
    #print(word, "->", stemmer.stem(word))

def getTag(t):
  if t.startswith("NN"):
    return "n"
  if t.startswith("JJ"):
    return "a"
  if t.startswith("RB"):
    return "r"
  if t.startswith("VB"):
    return "v"
  return "n"

#lemmatization
from nltk import pos_tag
from nltk.stem import WordNetLemmatizer
nltk.download('averaged_perceptron_tagger_eng')
nltk.download('wordnet')
lemmatizer = WordNetLemmatizer()
for sentence in sent_tokenize(input):
  for (word, tag) in pos_tag(word_tokenize(sentence)):
    print("Lemma("+word+") = ", lemmatizer.lemmatize(word,pos=getTag(tag)))
    #print("Lemma("+word+") = ", lemmatizer.lemmatize(word))

import nltk
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
lemmatizer = WordNetLemmatizer()
print(lemmatizer.lemmatize("singing"))

#stop word removal
from nltk.corpus import stopwords
nltk.download('stopwords')
sw = stopwords.words('english')
def removeStopWords(text):
  lowerCaseStopWords = []
  for word in sw:
    lowerCaseStopWords.append(word.lower())
  ret = ""
  for sentence in sent_tokenize(text):
    for word in word_tokenize(sentence):
      if word.lower() not in lowerCaseStopWords:
        ret += word + " "
  return ret

removeStopWords(input)

def posTagText(text):
  ret = ""
  for sentence in sent_tokenize(text):
    ret += str(pos_tag(word_tokenize(sentence))) + "\n"
  return ret

#stop word removal
#print(removeStopWords(input))
stopWordLessInput = removeStopWords(input)
print("Text: ", input)
print("Stop Word Less Text: ", stopWordLessInput)
posTaggedText = posTagText(input)
posTaggedSWLessText = posTagText(stopWordLessInput)
print("PoS Tagged Input:",posTaggedText)
print("PoS Tagged Stop Wordless Text:",posTaggedSWLessText)

#Spelling Correction
from nltk.metrics.distance import edit_distance
from nltk.corpus import words
nltk.download('words')
correct_spellings = words.words()

#Tweak 1 - Remove non-alphabetic words
#Tweak 2 - Tokenize first as sentences, then as words.

#1 assumption. A substitute is valid only if the edit distance is == 1.
for word in word_tokenize(sent_tokenize(input)[0]):
  if word in correct_spellings:
    print(word)
  else:
    candidates = []
    for w in correct_spellings:
      dist = edit_distance(word.lower(), w.lower())
      if dist < 2:
        candidates.append(w.lower())
    print(word + "->" + str(candidates))

taggedTokens = word_tokenize(input)
print(pos_tag(taggedTokens))
taggedTokenList = pos_tag(taggedTokens)

#Homework: Combine this error-checking approach with spell correction.

for sentence in sent_tokenize(input):
  taggedTokens = pos_tag(word_tokenize(sentence))
  for (token, tag) in taggedTokens:
    if tag.startswith("NNP"):
      print(token)
      continue
    if not tag.isalpha():
      print(token)
      continue
    if token.lower() in correct_spellings:
      print(token)
    else:
      candidates = []
      for w in correct_spellings:
        l1 = len(w)
        l2 = len(token)
        if abs(l1-l2) < 2:
          dist = edit_distance(w.lower(), token.lower())
          if(dist < 2):
            candidates.append(w)
      print(token + " -> " + str(candidates))

"""Some other improvements:
1. Fixing the punctuation marks.
2. Adding Context. This may involve relaxing our assumption of edit distance of 1.
"""